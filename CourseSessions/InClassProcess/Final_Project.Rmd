---
title: "Mashable.com - A Process to Predict Online News Popularity"
author: "Abhinandan Saini"
output:
  html_document:
    css: ../../AnalyticsStyles/default.css
    theme: paper
    toc: no
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    includes:
      in_header: ../../AnalyticsStyles/default.sty
always_allow_html: yes
---

This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the number of shares in social networks, i.e. how popular any given article is. The dataset is publicly available at [University of California Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Online+News+Popularity)

[Mashable Inc.](http://www.mashable.com) is a digital media website founded in 2005. It has been described as a "one stop shop" for social media. As of November 2015, it has over 6,000,000 Twitter followers and over 3,200,000 fans on Facebook.

```{r setuplibraries, echo=FALSE, message=FALSE}
suppressWarnings(source("../../AnalyticsLibraries/library.R"))
# Package options
suppressWarnings(ggthemr('fresh'))  # ggplot theme
opts_knit$set(progress=FALSE, verbose=FALSE)
opts_chunk$set(echo=FALSE, fig.align="center", fig.width=10, fig.height=6.35, results="asis")
options(knitr.kable.NA = '')
```

<hr>\clearpage

# The Data

First we load the data to use (see the raw .Rmd file to change the data file as needed):

```{r setupdata1E, echo=FALSE, tidy=TRUE}
# Please ENTER the name of the file with the data used. The file should be a .csv with one row per observation (e.g. person) and one column per attribute. Do not add .csv at the end, make sure the data are numeric.
datafile_name = "../Sessions23/data/OnlineNewsPopularity.csv"

# Please enter the minimum number below which you would like not to print - this makes the readability of the tables easier. Default values are either 10e6 (to print everything) or 0.5. Try both to see the difference.
MIN_VALUE = 0.5

# Please enter the maximum number of observations to show in the report and slides. 
# DEFAULT is 10. If the number is large the report may be slow.
max_data_report = 10
```

```{r}
ProjectData <- read.csv(datafile_name)
ProjectData <- data.matrix(ProjectData) 
ProjectData_INITIAL <- ProjectData

```

**Attribute Information in Dataset are as follows:**

0. **url:** URL of the article (non-predictive)
1. **timedelta:** Days between the article publication and the dataset acquisition (non-predictive)
2. **n_tokens_title:** Number of words in the title 
3. **n tokens content:** Number of words in the content 
4. **n_unique_tokens:** Rate of unique words in the content 
5. **n_non_stop_unique_tokens:** Rate of unique non-stop words in the content 
6. **num_hrefs:** Number of links 
7. **num_self_hrefs:** Number of links to other articles published by Mashable 
8. **num_imgs:** Number of images 
9. **num_videos:** Number of videos 
10. **average_token_length:** Average length of the words in the content 
11. **num_keywords:** Number of keywords in the metadata 
12. **self_reference_min_shares:** Min. shares of referenced articles in Mashable 
13. **self_reference_max_shares:** Max. shares of referenced articles in Mashable 
14. **self_reference_avg_sharess:** Avg. shares of referenced articles in Mashable 
15. **global_subjectivity:** Text subjectivity 
16. **global_sentiment_polarity:** Text sentiment polarity 
17. **global_rate_positive_words:** Rate of positive words in the content 
18. **global_rate_negative_words:** Rate of negative words in the content 
19. **rate_positive_words:** Rate of positive words among non-neutral tokens 
20. **rate_negative_words:** Rate of negative words among non-neutral tokens 
21. **title_subjectivity:** Title subjectivity 
22. **title_sentiment_polarity:** Title polarity 
23. **abs_title_subjectivity:** Absolute subjectivity level 
24. **abs_title_sentiment_polarity:** Absolute polarity level 
25. **shares:** Number of shares (target)

**Stop Words** usually refer to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools. For some search engines, these are some of the most common, short function words, such as the, is, at, which, and on.

<hr>\clearpage

# Key Customer Characteristics

```{r setupfactor, echo=FALSE, tidy=TRUE}
# Please ENTER then original raw attributes to use. 
# Please use numbers, not column names, e.g. c(1:5, 7, 8) uses columns 1,2,3,4,5,7,8
factor_attributes_used = c(3:25)

# Please ENTER the selection criterions for the factors to use. 
# Choices: "eigenvalue", "variance", "manual"
factor_selectionciterion = "eigenvalue"

# Please ENTER the desired minumum variance explained 
# (Only used in case "variance" is the factor selection criterion used). 
minimum_variance_explained = 65  # between 1 and 100

# Please ENTER the number of factors to use 
# (Only used in case "manual" is the factor selection criterion used).
manual_numb_factors_used = 8

# Please ENTER the rotation eventually used (e.g. "none", "varimax", "quatimax", "promax", "oblimin", "simplimax", and "cluster" - see help(principal)). Default is "varimax"
rotation_used = "varimax"

```

```{r}
factor_attributes_used <- intersect(factor_attributes_used, 1:ncol(ProjectData))
ProjectDatafactor <- ProjectData[,factor_attributes_used]
ProjectDatafactor <- ProjectData <- data.matrix(ProjectDatafactor)
```

## Steps 1-2: Check the Data 

Here is a sample of the first 250 rows of the Dataset:

```{r}
options(warn=-1)
library(googleVis)
library(dplyr)
library(lattice)
library(ggplot2)

local_directory <- getwd()
if (!exists("gadata1")) 
  gadata1 <- within(read.csv(paste(local_directory,"../Sessions23/data/OnlineNewsPopularity.csv", sep="/")),rm("X"))
t1 <- gvisTable(gadata1[1:250,],options = list(showRowNumber = FALSE, width = 800, height = min(400,27*(nrow(gadata1) + 1)), allowHTML = TRUE, page = 'disable'))
print(t1,'chart')
```


The data we use here have the following descriptive statistics: 

```{r}
iprint.df(round(my_summary(ProjectDatafactor), 2))
```

The data is Scaled and summary statistics are reprinted:

```{r, echo=FALSE, tidy=TRUE}
ProjectDatafactor_scaled=apply(ProjectDatafactor, 2, function(r) {if (sd(r)!=0) res=(r-mean(r))/sd(r) else res=0*r; res})
```

```{r}
iprint.df(round(my_summary(ProjectDatafactor_scaled), 2))
```


## Step 3: Check Correlations

```{r}
thecor = round(cor(ProjectDatafactor),2)
iprint.df(round(thecor,2), scale=TRUE)
```


## Step 4: Choose number of factors


```{r}
# Here is how the `principal` function is used 
UnRotated_results<-principal(ProjectDatafactor, nfactors=ncol(ProjectDatafactor), rotate="none",score=TRUE)
UnRotated_factors<-round(UnRotated_results$loadings,2)
UnRotated_factors<-as.data.frame(unclass(UnRotated_factors))
colnames(UnRotated_factors)<-paste("Comp",1:ncol(UnRotated_factors),sep="")
```

```{r}
# Here is how we use the `PCA` function 
Variance_Explained_Table_results<-PCA(ProjectDatafactor, graph=FALSE)
Variance_Explained_Table<-Variance_Explained_Table_results$eig
Variance_Explained_Table_copy<-Variance_Explained_Table

rownames(Variance_Explained_Table) <- paste("Component", 1:nrow(Variance_Explained_Table), sep=" ")
colnames(Variance_Explained_Table) <- c("Eigenvalue", "Pct of explained variance", "Cumulative pct of explained variance")
```

After running the Principal Component Analysis, we loook at the **variance explained** as well as the **eigenvalues** to choose the relevant number of factors:

```{r}
iprint.df(round(Variance_Explained_Table, 2))
```

```{r}
eigenvalues  <- Variance_Explained_Table[, "Eigenvalue"]
df           <- cbind(as.data.frame(eigenvalues), c(1:length(eigenvalues)), rep(1, length(eigenvalues)))
colnames(df) <- c("eigenvalues", "components", "abline")
iplot.df(melt(df, id="components"))
```

Based on the Principal Component Analysis, 6 factors out of the 23 are chosen.

## Step 5: Interpret the factors


```{r}
if (factor_selectionciterion == "eigenvalue")
  factors_selected = sum(Variance_Explained_Table_copy[,1] >= 1)
if (factor_selectionciterion == "variance")
  factors_selected = 1:head(which(Variance_Explained_Table_copy[,"cumulative percentage of variance"]>= minimum_variance_explained),1)
if (factor_selectionciterion == "manual")
  factors_selected = manual_numb_factors_used
```

We check the correlation of each of these six factors with the rest of the attributes.

```{r}
Rotated_results<-principal(ProjectDatafactor, nfactors=max(factors_selected), rotate=rotation_used,score=TRUE)
Rotated_factors<-round(Rotated_results$loadings,2)
Rotated_factors<-as.data.frame(unclass(Rotated_factors))
colnames(Rotated_factors)<-paste("Comp.",1:ncol(Rotated_factors),sep="")

sorted_rows <- sort(Rotated_factors[,1], decreasing = TRUE, index.return = TRUE)$ix
Rotated_factors <- Rotated_factors[sorted_rows,]

iprint.df(Rotated_factors, scale=TRUE)
```

To better visualize and interpret the factors we often "suppress" loadings with small values, e.g. with absolute values smaller than 0.5. In this case our factors look as follows after suppressing the small numbers:

```{r}
Rotated_Factors_thres <- Rotated_factors
Rotated_Factors_thres[abs(Rotated_Factors_thres) < MIN_VALUE]<-NA
colnames(Rotated_Factors_thres)<- colnames(Rotated_factors)
rownames(Rotated_Factors_thres)<- rownames(Rotated_factors)

iprint.df(Rotated_Factors_thres, scale=TRUE)
```


## Step 6:  Save factor scores

We can now either replace all initial variables used in this part with one of the initial variables for each of the selected factors in order to represent that factor. Here is how the factor scores  are for the first few respondents:

```{r}
NEW_ProjectData <- round(Rotated_results$scores[,1:factors_selected,drop=F],2)
colnames(NEW_ProjectData)<-paste("DV (Factor)",1:ncol(NEW_ProjectData),sep=" ")

iprint.df(t(head(NEW_ProjectData, 10)), scale=TRUE)
```

Where,

**DV (Factor) 1:** Rate of unique non-stop words in the content

**DV (Factor) 2:** Rate of negative (or positive) words in the content

**DV (Factor) 3:** Avg. shares of referenced articles in Mashable

**DV (Factor) 4:** Number of words in the content

**DV (Factor) 5:** Absolute polarity level in title

**DV (Factor) 6:** Number of videos in the article


<hr>\clearpage

By focusing on these six factors, Mashable should be able to better predict whether an article will shared on social media. Moreover, Mashable can potentially increase the number of shares for each article by setting the value of each of these attributes such that it maximizes the chance that a reader will share that article. 
